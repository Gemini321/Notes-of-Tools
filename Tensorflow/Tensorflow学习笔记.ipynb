{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow\n",
    "相比与Pytorch，Tensorflow更像是一门独立的语言，有着完整的生态。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "# import库文件\n",
    "from tensorflow.python.client import device_lib\n",
    "import tensorflow.compat.v1 as tf\n",
    "import numpy as np\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 5595381858228685045\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 16937379822857199316\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 6837873514266135847\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# 打印硬件信息\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All the available GPUs:\n",
      " [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "# 选择可用设备\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(\"All the available GPUs:\\n\", physical_devices)\n",
    "if physical_devices:\n",
    "    gpu = physical_devices[0]\n",
    "    tf.config.experimental.set_memory_growth(gpu,True)\n",
    "    tf.config.experimental.set_visible_devices(gpu, 'GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Welcome to the exciting world of Deep Neural Networks!'\n"
     ]
    }
   ],
   "source": [
    "# 打印字符串\n",
    "message = tf.constant(\"Welcome to the exciting world of Deep Neural Networks!\")\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    print(sess.run(message))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow基础\n",
    "Tensorflow的程序分为两部分：计算图的构建和生效。\n",
    "\n",
    "> 计算图的构建：添加变量和操作，并按照逐层建立神经网络的顺序传递张量  \n",
    "> 计算图的生效：使用tf.compat.v1.Session()定义一个会话对象，然后用run()方法运行。\n",
    "其中run(fetches, feed_dict=None, options=None, run_metadata)，运算结果的值在fetches中提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 3 8 7]\n"
     ]
    }
   ],
   "source": [
    "# 执行简单加法\n",
    "tf.device('/device:XLA_GPU:0')\n",
    "v_1 = tf.constant([1, 2, 3, 4]) # 添加变量\n",
    "v_2 = tf.constant([2, 1, 5, 3])\n",
    "v_add = tf.add(v_1, v_2) # v_add = v_1 + v_2，添加操作\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    print(sess.run(v_add)) # 运行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow的数据类型：\n",
    "> tf.int32, tf.float32, tf.float64  \n",
    "> tf.bool(tf.constant(\\[True\\]))  \n",
    "> tf.string(tf.constant(\"Hello world.\"))  \n",
    "\n",
    "Tensorflow支持的三种张量：\n",
    "> 常量：其值不能改变的量  \n",
    "> 变量：当一个量在会话中需要更新时使用。变量占用内存并需要初始化（通常表示权重和偏置），变量和初始化的分离，使所有关于图变量的赋值和计算都要通过tf.Session的run进行。初始化可以用sess.run(tf.global_variables_initializer())集体初始化，也可以单个初始化sess.run(x.initializer)  \n",
    "> 占位符：待定变量，需要在调用时用feed_dict传入，无需初始化（tf.placeholder()在tensorflow 2.0被移除）  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.]\n",
      "[[15.656019   6.9269133 14.252337   2.2867928 10.612704 ]\n",
      " [ 5.1965218  5.3856497  5.419628  17.658213   3.7272491]\n",
      " [ 4.484955  15.107275   9.457092  13.465555   7.025157 ]\n",
      " [14.972277   8.535312   6.8968334  6.6343575 12.186405 ]]\n"
     ]
    }
   ],
   "source": [
    "# 标量常量/向量\n",
    "t_1 = tf.constant(1) # 使用tf.constant(value, dtype)创建const Tensor\n",
    "t_2 = tf.fill([2, 3], 2) # tf.fill(dimension, value)\n",
    "t_3 = tf.zeros([2, 3], tf.int32) # t_3 = tf.constant([[0, 0, 0], [0, 0, 0]])\n",
    "t_4 = tf.ones([2, 3], tf.int32) # tf.ones(dimension, dtype=float32)\n",
    "list_np = np.arange(0, 5)\n",
    "t = tf.convert_to_tensor(t_np, dtype=tf.int32) # 使用tf.convert_to_tensor(value, dtype)将numpy数据类型转化为tensor\n",
    "\n",
    "# 变量，通过变量类创建\n",
    "rand_t = tf.random.uniform([10, 10], 0, 10, seed=0) # 形状为[10, 10]，范围在[0, 10)的随机均匀分布\n",
    "rand_s = tf.random.normal([10, 10], stddev=2) # 均值为0，标准差为2的正态分布\n",
    "rand_st =  tf.random.truncated_normal([10, 10], mean=0, stddev=2) # 均值为0，标准差为2且所有数分布在2*sigma间\n",
    "t_5 = tf.Variable(rand_t)\n",
    "weights = tf.Variable(tf.random.normal([100, 100], stddev=2)) # 定义形状为[100, 100]，符合正态分布的权重矩阵\n",
    "bias = tf.Variable(tf.zeros([100]), name='biases')\n",
    "weight2 = tf.Variable(weights.initialized_value(), name='w2') # 用变量初始化\n",
    "\n",
    "g1 = tf.Graph()\n",
    "with g1.as_default():\n",
    "    v = tf.get_variable(\"v\", initializer=tf.zeros_initializer()(shape=[1])) # 图定义时的变量初始化\n",
    "\n",
    "with tf.Session(graph=g1) as sess:\n",
    "    tf.initialize_all_variables().run()\n",
    "    with tf.variable_scope(\"\", reuse=True):\n",
    "        print(sess.run(tf.get_variable(\"v\")))\n",
    "\n",
    "# 占位符 tf.placeholder(dype, shape=None, name=None)\n",
    "x = tf.placeholder(\"float\")\n",
    "y = 2 * x\n",
    "data = tf.random.uniform([4, 5], 10) # 定义一个方法\n",
    "with tf.Session() as sess:\n",
    "    x_data = sess.run(data) # 创建会话图并对需要提取的张量显示使用运行命令\n",
    "    print(sess.run(y, feed_dict = {x: x_data}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.Variable(**initial_value=None**, trainable=True, collections=None, validate_shape=True, caching_device=None, **name=None**, variable_def=None, dtype=None, expected_shape=None, import_scope=None)  \n",
    "tf.get_variable(**name**, shape=None, dtype=None, **initializer=None**, regularizer=None, trainable=True, collections=None, caching_device=None, partitioner=None, validate_shape=True, custom_getter=None):\n",
    "\n",
    "> tf.Variable()在变量命名相同时自动处理为不同名字，而tf.get_variable()则会报错  \n",
    "> 当tf.get_variable()的同名变量出现在同一变量域内，且reuse=True，则可以进行变量共享；\n",
    "tf.get_variable()在tensorflow 2.0被移除\n",
    "\n",
    "tf.variable_scope(scope)和tf.name_scope(scope):\n",
    "\n",
    "> tf.variable_scope(scope)直接指定变量域，可以进行变量共享，与tf.get_variable()搭配使用。无论如何嵌套，若scope相同，直接跳转至第一次定义处，而且reuse=True无法变回False  \n",
    "> tf.name_scope(scope)指定命名域，对变量的使用没有影响，主要是用来区分对象所处的神经网络的结构，并且能在TensorBoard中展示出来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'conv1/w1:0' shape=() dtype=float32_ref> <tf.Variable 'conv1/w1:0' shape=() dtype=float32_ref>\n",
      "conv1_1/add\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.variable_scope('conv1'):\n",
    "    v1 = tf.get_variable('w1', initializer=1.0) # 注意：variable的默认类型为float32，直接赋1意味着类型为int32，会为后面的变量共享带来麻烦\n",
    "with tf.variable_scope('conv1', reuse=True):\n",
    "# 或写成tf.get_variable_scope().reuse_variables()，前者返回变量域，后者直接在变量域内修改reuse\n",
    "    v2 = tf.get_variable('w1')\n",
    "    x = v1 + v2\n",
    "print(v1, v2)\n",
    "print(x.op.name) # 算子所处变量域也会改变"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow的矩阵操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1.]]\n",
      "WARNING:tensorflow:From <ipython-input-15-c1bada25340f>:24: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession() # 创建会话图\n",
    "\n",
    "I_matrix = tf.eye(5) # 定义一个5x5单位矩阵\n",
    "print(I_matrix.eval()) # 单独print会显示该tensor的信息而非值\n",
    "\n",
    "X = tf.Variable(tf.eye(10))\n",
    "sess.run(X.initializer) # 变量初始化\n",
    "A = tf.Variable(tf.random.normal([5, 10]))\n",
    "sess.run(A.initializer)\n",
    "\n",
    "product = tf.matmul(A, X) # 矩阵乘法\n",
    "#print(product.eval())\n",
    "b = tf.Variable(tf.random_uniform([5, 10], 0, 2, dtype=tf.int32))\n",
    "sess.run(b.initializer)\n",
    "#print(b.eval())\n",
    "b_new = tf.cast(b, dtype=tf.float32) # 类型转换\n",
    "t_sum = tf.add(product, b_new) # 矩阵加法\n",
    "t_sub = product - b_new        # 矩阵减法\n",
    "#print(\"A * X + b:\\n\", t_sum.eval())\n",
    "#print(\"A * X - b:\\n\", t_sub.eval())\n",
    "\n",
    "ele_wise_product = A * A # 对应元素相乘\n",
    "scal_mul = tf.scalar_mul(2, A) # 标量乘法\n",
    "ele_wise_div = tf.div(X, X) # 对应元素相除\n",
    "\n",
    "min = tf.reduce_min(A) # 计算张量的最小值\n",
    "max = tf.reduce_max(A) # 计算张量的最大值\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    #print(sess.run(ele_wise_product))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorBoard可视化数据流图\n",
    "TensorFlow 使用 TensorBoard 来提供计算图形的图形图像。这使得理解、调试和优化复杂的神经网络程序变得很方便。TensorBoard 也可以提供有关网络执行的量化指标。它读取 TensorFlow 事件文件，其中包含运行 TensorFlow 会话期间生成的摘要数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 神经网络\n",
    "### Batch_size与Epoch_size\n",
    "\n",
    "> Batch_size:一次训练的样本数/进行一次调参所需的样本数  \n",
    "> Epoch_size:将所有样本迭代的次数\n",
    "\n",
    "区别：  \n",
    "> 未使用Batch_size时，默认将所有数据输入网络并进行反向传播。这样做使得计算梯度的方向最准确，但是这样将导致总的参数调整次数变少（Epoch_size不变的情况下）  \n",
    "> 随着Batch_size增大，方向越准确，但一次训练所需的内存增大，速度下降，一般随着Batch_size的增大需要适当增大Epoch_size来保证调参次数，同时学习率也要相应提高\n",
    "\n",
    "对应的算法：\n",
    "> BGD(Batch Gradient Decent，批量梯度下降法)：未使用Batch_size，梯度准确，非常耗时，可能因为神经网络是非凸的而收敛至局部最优点  \n",
    "> SGD(Stochastic Gradient Descent，随机梯度下降法)：令Batch_size = 1，梯度方向不稳定，难以朝着正确方向前进，因此需要增大Epoch_size来保证学习率  \n",
    "> mini-batch SGD：选择合适的Batch_size，缓解了GD算法直接掉入起始点附近的局部最优解的情况，同时由于梯度更准确，学习率需要相应提高"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 过拟合\n",
    "\n",
    "在深度神经网络中存在过拟合的情况：\n",
    "\n",
    "![过拟合](https://img-blog.csdnimg.cn/20190319100715848.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3h1YWhvMDkwNw==,size_16,color_FFFFFF,t_70)\n",
    "\n",
    "在神经网络中，防止过拟合有以下方法：\n",
    "\n",
    "#### 正则化\n",
    "\n",
    "简单来说，正则化是一种为了减小测试误差的行为(有时候会增加训练误差)。我们在构造机器学习模型时，最终目的是让模型在面对新数据的时候，可以有很好的表现。当你用比较复杂的模型比如神经网络，去拟合数据时，很容易出现过拟合现象(训练集表现很好，测试集表现较差)，这会导致模型的泛化能力下降，这时候，我们就需要使用正则化，降低模型的复杂度。\n",
    "\n",
    "模型的过拟合是因为考虑了过多的“不合适”样本点，这样，求导的时候，导数值很大，而自变量的值可大可小，所以意味着系数w要很大，所以当我们让w减少的时候，即意味着忽略这些样本点，即减小了模型的复杂度。\n",
    "\n",
    "$L^p$范数的公式：$||x||_p = (\\sum_{i} |x_i|^p)^{\\frac{1}{p}}$。通过对所有数据额外加上该范数，完成正则化：\n",
    "$\\overline{J}(\\omega, b) = J(\\omega, b) + \\frac{\\lambda}{2m} L^p(\\omega)$，其中$m$为样本个数而$\\lambda$为控制正则化的超参数。\n",
    "\n",
    "##### L1正则化\n",
    "\n",
    "L1正则化通常用于进行特征选择。在求导后，得到$\\omega_1 := \\omega_1 - \\alpha d\\omega_1 = \\omega_1 - \\frac{\\alpha\\lambda}{2m}sign(\\omega_1) - \\frac{\\partial J}{\\partial \\omega_1}$，若$\\omega_1$为整数则减去一个固定的常数而若为负数则减去一个常数，最终使某些只被激活几次的神经元收敛至0。这样使得被小部分不适合的样本点影响的神经元能快速收敛至0，从而防止过拟合。\n",
    "\n",
    "##### L2正则化\n",
    "\n",
    "L2正则化通常用于进行模型简化。每次更新$\\omega_1$时，需要$\\omega_1 = (1 - \\frac{\\alpha\\lambda}{m})\\omega_1 - \\frac{\\partial J}{\\partial\\omega_1}$，每次更新时都减去一个数，使得权值趋向于0而不变为0（权值衰减，weight decay）\n",
    "\n",
    "#### dropout\n",
    "\n",
    "在每次迭代后随机删除一部分的结点，使得结点之间的关联性与复杂度减小。在大数据集中效果更佳。\n",
    "\n",
    "#### earlystopping\n",
    "\n",
    "提前终止，防止过拟合。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
