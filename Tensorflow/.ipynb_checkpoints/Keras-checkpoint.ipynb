{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras\n",
    "\n",
    "Keras是一个以Tensorflow、Theano、CNTK为计算后台的深度学习建模环境，相较于其他工具具有高度模块化的特点，使用起来非常方便快捷。在使用时，可以根据自身需要切换后台，默认为Tensorflow。\n",
    "\n",
    "```Shell\n",
    "!cat ~/.keras/keras.json\n",
    "```\n",
    "\n",
    "Tensorflow中也实现了一套keras协议，使用更贴合Tensorflow的框架，但是只能使用Tensorflow进行运算，部分对象和函数的名称有所不同。本文将采用Keras进行讲解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras层 keras.layers\n",
    "\n",
    "所有层都有以下方法：\n",
    "\n",
    "> layer.get_weights()：返回层的权重（numpy array）  \n",
    "> layer.set_weights(weights)：从numpy array中将权重加载到该层\n",
    "> layer.get_config()：返回该层的配置信息\n",
    "\n",
    "### 卷积层 Convolutional Layer\n",
    "\n",
    "主要有Conv1D,Conv2D,Conv3D，下面以Conv2D为例：\n",
    "\n",
    "```Python\n",
    "keras.layers.convolutional.Conv2D(filters, kernel_size, strides=(1, 1), padding='vaild', activation=None, use_bias=True, kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, input_shape=None)\n",
    "```\n",
    "\n",
    "参数说明：\n",
    "> filters：卷积核个数  \n",
    "> kernel_size：卷积核大小（tuple类型，e.g.(5, 5)）  \n",
    "> strides：步长，默认为(1, 1)  \n",
    "> padding：补0原则，'valid'指卷积后大小可以与原大小不同，即不进行处理，'same'指卷积后大小相同  \n",
    "> 若为第一层，则需要加入input_shape\n",
    "\n",
    "注：卷积层在Tensorflow的keras中为tensorflow.keras.layers.Convolution2D()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 池化层 Pooling Layer\n",
    "\n",
    "主要有MaxPooling和AveragePooling两种，每种都有三个维度，下面只以2D为例。\n",
    "\n",
    "```python\n",
    "keras.layers.pooling.MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None)\n",
    "\n",
    "keras.layers.pooling.AveragePooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None)\n",
    "```\n",
    "\n",
    "参数说明：\n",
    "\n",
    "> pool_size：tuple形式，窗口大小；(2, 2)使原图片的长宽均变为原来的一半  \n",
    "> strides：整数或长为2的tuple  \n",
    "> padding：'same'或'valid'\n",
    "> data_format：'channels_first'或'channels_last'，未设置过默认为'channels_last'\n",
    "\n",
    "在'channels_last'模式下，需要输入(samples, channels, rows, cols)形式的4D tensor\n",
    "\n",
    "注：池化层在Tensorflow的keras中为tensorflow.keras.layers.MaxPooling2D()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 激活函数 Activations\n",
    "\n",
    "激活函数可以通过设置单独的激活层来实现，也可以在构造层对象时通过传递activation参数来实现：\n",
    "\n",
    "```python\n",
    "from keras.layer import Activation, Dense\n",
    "\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('tanh'))\n",
    "\n",
    "# 等价于model.add(Dense(64, activation='tanh'))\n",
    "```\n",
    "\n",
    "激活函数有：\n",
    "\n",
    "> softmax：$S_i = \\frac{e^i}{\\sum_{n}^{j=1}e^j}$，常用于分类，将一个神经元的输出映射到\\[0,1\\]，反向传播时 $y_{i}^{'} = y_i - 1$  \n",
    "> relu：$y_{i} = y_{i} > 0 ? y_{i} : 0$，简单快捷 ，值域\\[0,$\\infty$\\)  \n",
    "> sigmoid：$\\frac{1}{1 + e^{-x}}$，值域(0,1)，符合神经学放电率，常用于分类的输出层，但是由于双向饱和性与恒为正（一个神经元的权值总是整体变化）的性质，导致其作为隐藏层的效果不好  \n",
    "> tanh：$y_{i} = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$，值域\\[-1,1\\]，在隐藏层tanh的效果好于sigmoid，但计算比较复杂  \n",
    "> elu：$y_{i} = x > 0 ? x : \\alpha (e^x - 1)$  \n",
    "> selu(scaled exponential linear unit)：$y_{i} = x > 0 ? \\lambda x : \\lambda\\alpha (e^x - 1)$  \n",
    "> (leaky relu)：$y_i = x > 0 ? x : 0.01x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 全连接层 Fully Connected Layer\n",
    "\n",
    "全连接层实现计算$Y = activation(WX + bias)$，其中bias为同一个神经元的所有连接共享。\n",
    "\n",
    "```python\n",
    "keras.layers.core.Dense(units, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)\n",
    "```\n",
    "\n",
    "说明：\n",
    "\n",
    "> kernel为本层的权值矩阵，bias为偏置向量，只有当use_bias=True才会被添加\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout层\n",
    "\n",
    "对上一层的神经元随机选取一定比例进行失活，权值保留但不更新，防止过拟合。\n",
    "\n",
    "```python\n",
    "keras.layers.core.Dropout(rate, noise_shape=None, seed=None)\n",
    "```\n",
    "\n",
    "参数说明：\n",
    "\n",
    "> rate：\\[0,1\\]的浮点数，控制需要断开的神经元比例\n",
    "> noise_shape：整数张量\n",
    "> seed：随机数种子"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten层\n",
    "\n",
    "将输入展平，即将多维的输入变成一维的输出，但是不影响batch的大小。通常在卷积后全连接层前进行展平。\n",
    "\n",
    "```python\n",
    "keras.layers.core.Flatten()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape层\n",
    "\n",
    "将输入转换成特定shape。\n",
    "\n",
    "```python\n",
    "keras.layers.core.Reshape(target_shape)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Permute层\n",
    "\n",
    "将输入的维度按照特定模式重排。\n",
    "\n",
    "```python\n",
    "keras.layers.core.Permute(dims)\n",
    "```\n",
    "\n",
    "dims：整数tuple，指定重排的模式，不包含样本数的维度。例如(2, 1)表示将第二个维度变成第一个维度，第一个维度变成第二个维度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras 函数\n",
    "\n",
    "### 回调函数 Callbacks\n",
    "\n",
    "在每个training/epoch/batch结束时，如果我们想执行某些任务，例如模型缓存、输出日志、计算当前的auc等等，Keras中的Callback就派上用场了。Callback能将每次训练后的参数和结果，方便我们在训练结束后调用或提前终止训练。\n",
    "\n",
    "在实际使用中，我们需要继承Keras的Callback类并重写``on_train_begin``, ``on_train_end``, ``on_epoch_begin``, ``on_epoch_end``, ``on_batch_begin``, ``on_batch_end``，来实现自己的Callback函数，并通过``self.model``访问模型本身，``self.params``访问训练参数。\n",
    "\n",
    "```python\n",
    "import keras\n",
    " \n",
    "# 定义callback类\n",
    "class MyCallback(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "        return\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}): # batch 为index, logs为当前batch的日志acc, loss...（记录损失值）\n",
    "        self.losses.append(logs.get('loss')) \n",
    "        return\n",
    "\n",
    "# 定义模型model\n",
    "...\n",
    "...\n",
    "\n",
    "# 调用callback\n",
    "cb = MyCallback()\n",
    "\n",
    "# 训练模型\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=10, callbacks=[cb])\n",
    "\n",
    "# 查看callback内容\n",
    "cb.losses\n",
    "```\n",
    "\n",
    "使用Callback函数可以帮助我们更快地选择最佳的epoch而不用对每个epoch都进行一次训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras model.fit() 函数\n",
    "\n",
    "```python\n",
    "model.fit(x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None, validation_freq=1)\n",
    "```\n",
    "\n",
    "参数说明：\n",
    "\n",
    "> x：输入数据，若只有一个输入，则类型为numpy array；若有多个输入，则x类型应为list，而list的元素numpy array  \n",
    "> y：输出数据，类型为numpy array  \n",
    "> batch_size：整数，指每个batch中的样本数，每个batch训练结束后进行一次参数修改  \n",
    "> epochs：整数，指终止训练时每个样本参与训练的次数，未设置initial_epoch时，为总训练轮数，否则训练轮数为epochs - initial_epoch（一轮指将所有样本训练一次）  \n",
    "> verbose：日志显示，0为不在标准输出流输出日志信息，1为输出进度条记录，2为每个epoch输出一行记录  \n",
    "> callbacks：list，其中的元素为keras.callbacks.Callback的对象，回调函数会在训练的适当时机被调用  \n",
    "> validation_split：0~1之间的浮点数，用来指定训练集的一定比例数据作为验证集。验证集不参与训练，且在每个epoch结束后验证模型指标，如损失函数和精确度等。注意validation_split在shuffle前，若数据本书有序，需要先手工打乱再指定validation_split。  \n",
    "> validation_data：形式为(X, y)的tuple，是指定的验证集，将覆盖validation_split。  \n",
    "> shuffle：boolen或string，表示是否将样本随机打乱，若为'batch'，则在每个batch内打乱  \n",
    "> class_weight：字典，将不同类型映射为不同权值，用于训练时调整损失函数  \n",
    "> sample_weight：权值的numpy array，用于训练时调整损失函数。  \n",
    "> initial_epoch：使该参数从指定的epoch开始训练，可以继续之前的训练。\n",
    "\n",
    "model.fit()函数返回一个History对象，该对象记录了损失函数和其他指标随epoch变化的情况。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras 保存模型\n",
    "\n",
    "在Keras中，有保存模型和加载模型的API，直接调用即可。\n",
    "\n",
    "```python\n",
    "# 模型保存\n",
    "print(\"Saving model.\")\n",
    "path = \"C:/Desktop/model.h5\"\n",
    "model.save(path)\n",
    "print(\"Successfully saved model.\")\n",
    "\n",
    "# 模型加载\n",
    "from keras.models import load_model\n",
    "print(\"Loading model.\")\n",
    "path = \"C:/Desktop/model.h5\"\n",
    "model = load_model(path)\n",
    "print(\"Successfully loaded model.\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
